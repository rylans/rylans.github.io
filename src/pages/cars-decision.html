<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width">
    <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Chivo:900">
    <link rel="stylesheet" type="text/css" href="../stylesheets/stylesheet.css" media="screen">

    <link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <title>Self-driving cars and decision theory / Rylan</title>
  </head>

  <body>
    <div id="container">
      <div class="inner">

        <header>
          <h1>Self-driving cars and decision theory</h1>
          <h2>How autonomous vehicles present an ethical dilemma</h2>
	  <h6>2015-05-28 &mdash; Rylan Santinon</h6>
        </header>

        <hr class="hrpng">

        <section id="main_content">
          <h3>
<a name="killchoice" class="anchor" href="#killchoice"><span class="octicon octicon-link"></span></a>The Killing Choice</h3>

<p>
I'd like to explore the implications of autonomous vehicles and decision theory.
Consider this dilemma posed by Lin:</p>

<blockquote>
 Imagine in some distant future, your autonomous car encounters this terrible choice:
 it must either swerve left and strike an eight-year old girl, or swerve right and strike an
 80-year old grandmother [33]. Given the car’s velocity, either victim would surely be killed
 on impact. If you do not swerve, both victims will be struck and killed; so there is good
 reason to think that you ought to swerve one way or another.
</blockquote>

<p>
In this case, the decision making agent (the autonomous car) must make a killing decision. It must
choose to kill the girl or the old lady. There is no viable third option. Failure to make a decision
is deciding to do nothing. Deciding to do nothing involves the death of two people instead of
the death of one.
</p>

<p>
You can't even get out of this dilemma by saying that the car should cede control the person sitting
in the driver seat because humans have slower reaction times and may need several seconds to get
reoriented (Lin). It would be extremely negligent to hand over control to a human every time
a difficult problem occurs. This would ensure the worst possible outcome in this case.
</p>

<p>
In order to formalize a decision problem, we would need to break up the problem into the possible
actions and the possible states that results from those actions. A rational agent is expected
to choose the action that results in the best outcome. When probabilties are involved, the agent
should make the appropriate expected utility calculation. We would expect an ethical decision making
agent to take human lives into account when making this crash-avoidance decision which would require
assigning utilities all three of these crash outcomes.
</p>

<p>
I suppose the naive way of answering the question is to say that children are less massive than
adults and colliding with less massive objects causes less injury to the passengers of the car. By
this line of reasoning, the decision would be to swerve and kill the girl.
</p>

<p>
This answer is naive because it misses the crux of the problem. The problem is that a decision making
agent must make a decision based on superficial features of an object in real time. Suppose that instead of an old lady and a young girl, we have a 35kg boy and a 35kg girl standing on the road. 
</p>

<p>
What would you choose? Would you kill the little boy or the little girl? 'Neither' is not an answer. 'Neither' means
'both.' You must make a decision. Failure to make a decision is deciding to do nothing. Deciding to do nothing
is letting both die. And don't tell me that you'd flip a coin. If the result of the coin flip tells you
to kill one of them then you still need to take responsibility for that kill. You may not relinquish 
responsiblity by leaving it up to luck or chance. Decide now.
</p>

<p>
This problem seems strikingly similar to Buridan's paradox. Which is posed
as follows: A hungry donkey standing between two bales of hay which are the
same distance away from the animal. Both bales of hay look equally delicious so the
donkey can't make a decision and he dies of starvation.
</p>

<p>
The hay on one side represents the girl, the hay on the other side represents the boy. Starvation
represents letting both die.
</p>

<p>
These are the kinds of decisions that self-driving cars need to make. They have to make
decisions that involve life, death and suffering. Decision-making is hard.
</p>


<h3>
<a name="decisionfactors" class="anchor" href="#decisionfactors"><span class="octicon octicon-link"></span></a>Decision Factors</h3>

<p>
Let's consider a similar problem: A vehicle is driving down the road and a deer unexpectedly
jumps out in front of the car. The autonomous vehicle could decide to swerve briefly
into oncoming traffic in order to avoid colliding with the deer and hurting the passengers.
What factors should the vehicle consider when making this decision?
</p>

<p>
Lin mentions the following decision factors:
</p>

<blockquote>
 the road-shoulder type
 (paved, gravel, none, etc.), the condition of the car’s tires and brakes, whether the car’s
 occupants are seat-belted, whether the car is transporting dangerous cargo that could spill
 or explode, proximity to hospital or emergency rescue, damage to property such as houses
 and buildings...
</blockquote>

<p>
Here are some more factors that could be relevent to making an optimal decision:
</p>

<ul>
  <li>The number of occupants inside of the vehicle</li>
  <li>The number of occupants inside of the oncoming vehicle</li>
  <li>The age and frailty of the occupants</li>
  <li>Whether any of the occupants are pregnant</li>
  <li>What stage of pregancy and how many fetuses are involved</li>
  <li>The blood types of the occupants</li>
  <li>The number of cats, dogs and animals in the vehicles</li>
  <li>The road conditions: wet, dry, icy</li>
  <li>The safety rating of both vehicles</li>
</ul>

<p>
Making this decision is not trivial. This involves gathering information with
an array of sensors and making a decision that will involve some deaths and
some permanent paralysis. Would you rather a human make this decision or
a computer? 
</p>

<p>
This isn't just about self-driving cars and transportation. This is about any
decision-making agent that interacts with humans.
</p>

<h3>
<a name="learningtodecide" class="anchor" href="#learningtodecide"><span class="octicon octicon-link"></span></a>Learning to Decide</h3>

<p>
The solution is not allowing the programmers to keep a database of situations with
the 'ethical' decision for each one. Morality is not a lookup table.
</p>

<p>
Goodall proposes a learning-based solution: 
</p>

<blockquote>
  A similar technique
  could be used, with much more training data, to understand how
  humans choose to behave—or should behave—in morally complex
  driving situations when time is not a factor. The neural network could
  be trained on a combination of simulation and recordings of crashes
  and near crashes, with human feedback on the ethical response
</blockquote>

<p>
This technique seems to be in the right direction, but it has a major flaw. The ethical
framework that is learnt would only be as ethical as a human. We should expect more
from computer systems. I don't want the vehicle to make the humanly ethical decision. I want
the vehicle to make the optimally ethical decision.
</p>

<p>
It seems that as a society, we're totally inept at answering these questions. The reason
why we still have debates about the death penalty, abortion and euthanasia is that we
don't have a coherent ethical framework concerning the value of human life and the 
cost of human suffering.
</p>

          <hr class="hrpng">
          <h5>References</h5>

          <ol class="refs">
              <li>Goodall, "Ethical Decision Making During Automated Vehicle Crashes" (http://people.virginia.edu/~njg2q/ethics.pdf)</li>
              <li>Goodall, "Vehicle Automation and the Duty to Act" (http://people.virginia.edu/~njg2q/dutytoact.pdf)</li>
              <li>Lin, "Why Ethics Matters for Autonomous Cars"</li>
          </ol>

        </section>

<footer class="hrpng">
<a href="../index.html">&lt; Back</a>
</footer>

      </div>
    </div>
  </body>
</html>
